# Ollama API é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

Ollama æ˜¯ä¸€ä¸ªæœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œå¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå„ç§å¼€æºæ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥ï¼Œå®Œå…¨å…è´¹ä¸”ä¿æŠ¤éšç§ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Ollama

#### Windows
```bash
# ä½¿ç”¨ PowerShell
iwr -useb get-ollama.com | iex

# æˆ–ä½¿ç”¨ Chocolatey
choco install ollama

# æˆ–ä½¿ç”¨ Scoop
scoop install ollama
```

#### macOS
```bash
# ä½¿ç”¨ Homebrew
brew install ollama

# æˆ–ä½¿ç”¨ curl
curl -fsSL https://ollama.com/install.sh | sh
```

#### Linux
```bash
# ä½¿ç”¨ curl
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–ä½¿ç”¨ apt
sudo apt update && sudo apt install ollama
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½ Llama 3.1 8Bï¼ˆæ¨èï¼‰
ollama pull llama3.1

# ä¸‹è½½ Qwen 2.5 7Bï¼ˆä¸­æ–‡æ¨èï¼‰
ollama pull qwen2.5:7b

# ä¸‹è½½ Mistral 7B
ollama pull mistral:7b

# ä¸‹è½½ Phi-3 Miniï¼ˆè½»é‡çº§ï¼‰
ollama pull phi3:mini

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
ollama list
```

### 3. å¯åŠ¨ Ollama æœåŠ¡

```bash
# å¯åŠ¨ Ollama API æœåŠ¡ï¼ˆé»˜è®¤ç«¯å£ 11434ï¼‰
ollama serve

# è‡ªå®šä¹‰ç«¯å£
OLLAMA_HOST=0.0.0.0 OLLAMA_PORT=11435 ollama serve

# åå°è¿è¡Œï¼ˆLinux/macOSï¼‰
nohup ollama serve > ollama.log 2>&1 &

# Windows åå°è¿è¡Œ
Start-Process -FilePath "ollama" -ArgumentList "serve" -WindowStyle Hidden
```

### 4. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# æµ‹è¯•èŠå¤©æ¥å£
curl http://localhost:11434/api/chat -X POST -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "Hello!"}],
  "stream": false
}'
```

## ğŸ“¦ å¯ç”¨æ¨¡å‹

### æ¨èæ¨¡å‹

| æ¨¡å‹åç§° | æè¿° | å¤§å° | ä¸Šä¸‹æ–‡é•¿åº¦ | æ¨èåœºæ™¯ |
|---------|------|------|-----------|----------|
| **llama3.1** | Meta Llama 3.1 8B | 4.7GB | 128K | é€šç”¨å¯¹è¯ã€å¤æ‚ä»»åŠ¡ |
| **qwen2.5:7b** | é€šä¹‰åƒé—® 2.5 7B | 4.5GB | 32K | ä¸­æ–‡å¯¹è¯ã€æ–‡æœ¬ç”Ÿæˆ |
| **mistral:7b** | Mistral 7B | 4.1GB | 32K | å¿«é€Ÿæ¨ç†ã€é€šç”¨å¯¹è¯ |
| **phi3:mini** | Phi-3 Mini 3.8B | 2.3GB | 128K | å¿«é€Ÿæ¨ç†ã€è¾¹ç¼˜è®¾å¤‡ |
| **gemma2:9b** | Google Gemma 2 9B | 5.5GB | 8K | é€šç”¨å¯¹è¯ã€æ¨ç† |

### ä¸“ä¸šæ¨¡å‹

| æ¨¡å‹åç§° | æè¿° | å¤§å° | ä¸Šä¸‹æ–‡é•¿åº¦ | æ¨èåœºæ™¯ |
|---------|------|------|-----------|----------|
| **llama3.1:70b** | Meta Llama 3.1 70B | 40GB | 128K | å¤æ‚æ¨ç†ã€é«˜è´¨é‡å¯¹è¯ |
| **qwen2.5:14b** | é€šä¹‰åƒé—® 2.5 14B | 9GB | 32K | å¤æ‚ä»»åŠ¡ã€å¤šè½®å¯¹è¯ |
| **mixtral:8x7b** | Mistral Mixtral 8x7B | 26GB | 32K | å¤æ‚æ¨ç†ã€é«˜è´¨é‡è¾“å‡º |
| **deepseek-coder-v2** | DeepSeek Coder V2 16B | 9GB | 16K | ä»£ç ç”Ÿæˆã€ç¼–ç¨‹è¾…åŠ© |
| **codellama:13b** | CodeLlama 13B | 7.4GB | 16K | ä»£ç ç”Ÿæˆã€ç¼–ç¨‹è¾…åŠ© |

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®ï¼š

```env
# Ollama APIé…ç½®ï¼ˆæœ¬åœ°ï¼‰
# Ollamaæ˜¯æœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·
# é»˜è®¤åœ°å€ï¼šhttp://localhost:11434/api
# å¦‚æœOllamaè¿è¡Œåœ¨å…¶ä»–ç«¯å£æˆ–æœåŠ¡å™¨ï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„URL
OLLAMA_API_URL=http://localhost:11434/api
```

### è‡ªå®šä¹‰ç«¯å£

```bash
# ä¿®æ”¹ Ollama æœåŠ¡ç«¯å£
export OLLAMA_PORT=11435
ollama serve

# æˆ–åœ¨å¯åŠ¨æ—¶æŒ‡å®š
OLLAMA_HOST=0.0.0.0 OLLAMA_PORT=11435 ollama serve
```

### è¿œç¨‹è®¿é—®

```bash
# å…è®¸è¿œç¨‹è®¿é—®ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
OLLAMA_HOST=0.0.0.0 ollama serve

# ä½¿ç”¨ Nginx åå‘ä»£ç†
# /etc/nginx/sites-available/ollama
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### JavaScript/Node.js

```javascript
import { createCalendarChatService } from './src/services/chatService.js';

const chatService = createCalendarChatService();
await chatService.init();

// ä½¿ç”¨ Ollama æ¨¡å‹
const result = await chatService.sendMessage('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', {
  model: 'llama3.1'
});

console.log(result.content);
```

### cURL

```bash
# åŸºæœ¬èŠå¤©è¯·æ±‚
curl http://localhost:11434/api/chat -X POST -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ—¥å†åŠ©æ‰‹"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
  ],
  "stream": false
}'

# æµå¼èŠå¤©è¯·æ±‚
curl http://localhost:11434/api/chat -X POST -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "è®²ä¸ªç¬‘è¯"}],
  "stream": true
}'
```

### Python

```python
import requests

url = "http://localhost:11434/api/chat"
headers = {"Content-Type": "application/json"}
data = {
    "model": "llama3.1",
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ—¥å†åŠ©æ‰‹"},
        {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
    ],
    "stream": False
}

response = requests.post(url, json=data, headers=headers)
print(response.json())
```

## ğŸ¯ å‚æ•°è¯´æ˜

### æ¨¡å‹å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|---------|------|
| `model` | string | `llama3.1` | æ¨¡å‹åç§° |
| `temperature` | number | `0.7` | æ§åˆ¶éšæœºæ€§ï¼ˆ0.0-2.0ï¼‰ |
| `max_tokens` | number | `1000` | æœ€å¤§ç”Ÿæˆtokensæ•° |
| `top_p` | number | `0.9` | æ ¸é‡‡æ ·å‚æ•°ï¼ˆ0.0-1.0ï¼‰ |
| `frequency_penalty` | number | `0.5` | é¢‘ç‡æƒ©ç½šï¼ˆ-2.0-2.0ï¼‰ |
| `presence_penalty` | number | `0.5` | å­˜åœ¨æƒ©ç½šï¼ˆ-2.0-2.0ï¼‰ |
| `stream` | boolean | `false` | æ˜¯å¦æµå¼è¾“å‡º |

### å‚æ•°è°ƒä¼˜å»ºè®®

```javascript
// åˆ›é€ æ€§å†™ä½œ
{
  temperature: 1.0,
  top_p: 0.95,
  frequency_penalty: 0.7,
  presence_penalty: 0.7
}

// äº‹å®æ€§å›ç­”
{
  temperature: 0.3,
  top_p: 0.8,
  frequency_penalty: 0.3,
  presence_penalty: 0.3
}

// ä»£ç ç”Ÿæˆ
{
  temperature: 0.2,
  top_p: 0.9,
  frequency_penalty: 0.1,
  presence_penalty: 0.1
}

// ä¸­æ–‡å¯¹è¯
{
  temperature: 0.7,
  top_p: 0.9,
  frequency_penalty: 0.5,
  presence_penalty: 0.5
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹é€‰æ‹©

```bash
# è½»é‡çº§æ¨¡å‹ï¼ˆå¿«é€Ÿå“åº”ï¼‰
ollama pull phi3:mini
# é€‚åˆï¼šå¿«é€Ÿå¯¹è¯ã€è¾¹ç¼˜è®¾å¤‡

# ä¸­ç­‰æ¨¡å‹ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰
ollama pull llama3.1
# é€‚åˆï¼šé€šç”¨åœºæ™¯ã€æ—¥å¸¸ä½¿ç”¨

# å¤§å‹æ¨¡å‹ï¼ˆé«˜è´¨é‡è¾“å‡ºï¼‰
ollama pull llama3.1:70b
# é€‚åˆï¼šå¤æ‚ä»»åŠ¡ã€é«˜è´¨é‡è¦æ±‚
```

### GPU åŠ é€Ÿ

```bash
# æ£€æŸ¥ GPU æ”¯æŒ
ollama --version

# ä½¿ç”¨ GPU è¿è¡Œï¼ˆéœ€è¦ CUDAï¼‰
CUDA_VISIBLE_DEVICES=0 ollama serve

# å¤š GPU é…ç½®
CUDA_VISIBLE_DEVICES=0,1 ollama serve
```

### å†…å­˜ä¼˜åŒ–

```bash
# è®¾ç½®æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
OLLAMA_NUM_CTX=8192 ollama serve

# é™åˆ¶å¹¶å‘è¯·æ±‚æ•°
OLLAMA_NUM_PARALLEL=2 ollama serve

# ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆæ›´å°å†…å­˜ï¼‰
ollama pull llama3.1:8b-q4_K_M
```

## ğŸ”’ å®‰å…¨å»ºè®®

### æœ¬åœ°è®¿é—®

```bash
# ä»…ç›‘å¬æœ¬åœ°
OLLAMA_HOST=127.0.0.1 ollama serve

# ä½¿ç”¨é˜²ç«å¢™é™åˆ¶è®¿é—®
# Linux
sudo ufw allow from 192.168.1.0/24 to any port 11434

# Windows
New-NetFirewallRule -DisplayName "Ollama" -Direction Inbound -LocalPort 11434 -Protocol TCP -Action Allow
```

### èº«ä»½éªŒè¯

```bash
# ä½¿ç”¨ Nginx åŸºæœ¬è®¤è¯
# /etc/nginx/sites-available/ollama
server {
    listen 80;
    server_name your-domain.com;

    auth_basic "Restricted";
    auth_basic_user_file /etc/nginx/.htpasswd;

    location / {
        proxy_pass http://localhost:11434;
    }
}

# åˆ›å»ºå¯†ç æ–‡ä»¶
sudo htpasswd -c /etc/nginx/.htpasswd user
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. è¿æ¥è¢«æ‹’ç»**
```bash
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
netstat -tuln | grep 11434  # Linux/macOS
netstat -an | findstr 11434  # Windows
```

**2. æ¨¡å‹æœªæ‰¾åˆ°**
```bash
# åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹
ollama list

# ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
ollama pull llama3.1
```

**3. å†…å­˜ä¸è¶³**
```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
ollama pull llama3.1:8b-q4_K_M

# å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
OLLAMA_NUM_CTX=4096 ollama serve

# å…³é—­å…¶ä»–åº”ç”¨é‡Šæ”¾å†…å­˜
```

**4. å“åº”ç¼“æ…¢**
```bash
# æ£€æŸ¥ GPU ä½¿ç”¨
nvidia-smi  # NVIDIA
rocm-smi  # AMD

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull phi3:mini
```

### æ—¥å¿—è°ƒè¯•

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
OLLAMA_DEBUG=1 ollama serve

# æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
tail -f ollama.log

# Windows äº‹ä»¶æŸ¥çœ‹å™¨
eventvwr.msc
```

## ğŸ“š å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£
- [Ollama å®˜ç½‘](https://ollama.com)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)

### æ¨¡å‹èµ„æº
- [Hugging Face](https://huggingface.co/models)
- [Ollama æ¨¡å‹åº“](https://ollama.com/library)

### ç¤¾åŒºæ”¯æŒ
- [Ollama Discord](https://discord.gg/ollama)
- [Ollama Reddit](https://reddit.com/r/ollama)

## ğŸ’¡ æœ€ä½³å®è·µ

1. **æ¨¡å‹é€‰æ‹©**
   - æ—¥å¸¸ä½¿ç”¨ï¼š`llama3.1` æˆ– `qwen2.5:7b`
   - å¿«é€Ÿå“åº”ï¼š`phi3:mini`
   - é«˜è´¨é‡ï¼š`llama3.1:70b`
   - ä»£ç ç”Ÿæˆï¼š`deepseek-coder-v2`

2. **å‚æ•°è°ƒä¼˜**
   - åˆ›é€ æ€§ä»»åŠ¡ï¼šæé«˜ temperatureï¼ˆ0.8-1.2ï¼‰
   - äº‹å®æ€§ä»»åŠ¡ï¼šé™ä½ temperatureï¼ˆ0.2-0.5ï¼‰
   - ä»£ç ç”Ÿæˆï¼šä½¿ç”¨ä½ temperatureï¼ˆ0.1-0.3ï¼‰

3. **èµ„æºç®¡ç†**
   - å®šæœŸæ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹
   - ç›‘æ§å†…å­˜å’Œ GPU ä½¿ç”¨
   - ä½¿ç”¨é‡åŒ–æ¨¡å‹èŠ‚çœå†…å­˜

4. **å®‰å…¨è€ƒè™‘**
   - ä»…åœ¨å—ä¿¡ä»»çš„ç½‘ç»œä¸­æš´éœ² API
   - ä½¿ç”¨èº«ä»½éªŒè¯ä¿æŠ¤è¿œç¨‹è®¿é—®
   - å®šæœŸæ›´æ–° Ollama ç‰ˆæœ¬

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2026å¹´1æœˆ14æ—¥  
**ç»´æŠ¤äºº**ï¼šå¼€å‘å›¢é˜Ÿ

**ğŸ‰ é‡è¦æç¤º**ï¼šOllama å®Œå…¨å…è´¹ä¸”æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ API å¯†é’¥ï¼Œä¿æŠ¤éšç§ï¼
